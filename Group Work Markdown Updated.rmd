---
title: "Medical Body Data Analysis"
author: "Abrar Ahmed"
date: "11/04/2021"
output: word_document
---

For this assignment, we have all attempted the questions individually and then came 
together to compare our answers. When we found that we had different ways of doing 
the same thing, we include each method and discuss which we might prefer. 

Firstly bringing the data set into R:
```{r setup, include=FALSE}
library(tidyverse)
bodydata = read_csv("bodyfat_sample.csv")
View(bodydata)
```

#Removing the men with id in {31,39,42,86}
Between us, we had two methods of doing this:
1) Remove the outliers via R index number: bodydata = (bodydata[ -c(5, 6, 7,13), ])
2) Remove via id number. We discussed and felt that for larger data sets, option 1 may not be the 
best as it could take too long to find the index number for each outlier you want to remove. 
```{r}
bodydata = bodydata[ !(bodydata$id %in% c(31,39,42,86)), ]
view(bodydata)
```

#Sorting in descending order according to body fat percentage
```{r}
bodydata = arrange(bodydata, -body_fat_percentage)
view(bodydata)
```

#Setting up the data sets that will be used through out the portfolio
```{r}
#When only the 9 circumferences measurements are needed
bodydata_9 = select(bodydata,-id,-body_fat_percentage,-age, -weight, -height)
view(bodydata_9)

#when age, weight and height need to be included too
bodydata_12 = select(bodydata,-id,-body_fat_percentage)
view(bodydata_12)

#when body fat percentage needs to be included too
bodydata_13 = select(bodydata,-id)
view(bodydata_13)
```

#TASK 1 - PCA

##PCA using the 9 circumference measurements

```{r}
#PCA using the nine body circumference measurements WITHOUT scaling
par(mar=c(3,3,3,0))
pca_results = prcomp(bodydata_9)
summary(pca_results)

#Biplot
library(ggfortify)
autoplot(pca_results, data=bodydata_9, label=TRUE, shape=FALSE,
         loadings=TRUE, loadings.label=TRUE)+
labs(title = "PCA Biplot for the 9 circumference measurements - unscaled")
```

This plot shows that we must scale due to the large measurements for chest hip and thigh compared to the other circumferences. Thus, we have decided to scale the data as the measurements vary by orders of magnitude.

```{r}
#PCA using the nine body circumference measurements WITH scaling
pca_results = prcomp(bodydata_9,scale.=TRUE)
summary(pca_results)

#Biplot
autoplot(pca_results, data=bodydata_9, label=TRUE, shape=FALSE,
         loadings=TRUE, loadings.label=TRUE)+
labs(title = "PCA Biplot for the 9 circumference measurements - scaled")


# Loadings plot
loadings = as.data.frame(pca_results$rotation[,1:4])
loadings$Symbol = row.names(loadings)
loadings = gather(loadings, key='Component', value='Weight', -Symbol)
ggplot(loadings, aes(x=Symbol,y=Weight)) +
  geom_bar(stat='identity') +
  facet_grid(Component~.)+
  labs(title = "PCA Loading plot for the 9 circumference measurements")

#Scree Plot method 1
#install.packages("factoextra")
library(factoextra)
p=fviz_screeplot(pca_results, addlabels = TRUE,
                 linecolor ="red",ncp = 9)
p + labs(title = "Scree Plot for 9 circumference Measurements (Method 1)", x = "Principal Components", y = "% of variance explained")


#Scree Plot method 2
variance_df = data.frame(PC= paste0("PC",1:9),
                         var_explained=(pca_results$sdev)^2/sum((pca_results$sdev)^2))
head(variance_df)
variance_df %>%
  ggplot(aes(x=PC,y=var_explained, group=1))+
  geom_col()+
  geom_point(size=4)+
  geom_line()+
  labs(title= "Scree Plot for the for the 9 circumference measurements (Method 2)")

#Scatter matrix
library(GGally)
ggpairs(bodydata_13)
```
We had 2 methods for making the screeplot. We think first is better as it required less bits of code. 

1. According to both the PCA biplot and Scree plot for the 9 circumference measurements, 64.79% and 12.62% of the variance is explained by PC1 and PC2 respectively. Thus these two principal components account for 77.4% of the variance. Therefore, we analyse the biplot for PC1 and PC2 first as it gives the most variance explained.

2. According to the PCA Biplot for 9 circumference measurements, we analyse that the hip and wrist measurements likely have a correlation between them since they are very close to each other on the biplot. We observe this for forearm and biceps too. Ankle and neck on the other hand are almost perpendicular to each other, which indicates that there is less correlation between them.This can be confirmed by looking at the scatter matrix for the variables. It shows that there is a significant moderate positive correlation between hip and wrist (r=0.699) and also between forearm and biceps (r=0.620) whilst the correlation between neck and ankle is weak and not significant (r = 0.205). The PCA Biplot for the 9 measurements can also be interpreted in a practical sense i.e. on a physical body, the hip is close to the wrist whilst the neck and ankle are far apart from each other. Thus, the variables on the Biplot seem to be sequenced as a body structure from neck to ankle represented from bottom to top on the biplot. 

3. Another interpretation of the biplot could be to look at PC1 and PC2 separately. We notice that PC1 seems to take into account roughly equal amounts of each body part and is therefore considering the average size of the body. PC2 on the other hand may be considering how fleshy or bony body parts are as we notice that the fleshier parts point down (e.g. neck, bicep, chest) and the bonier parts point up (e.g. ankle, knee). 

4. We explored this further looking at participants that were high and low on the axis for PC1. 4 is low and 34 is high. Looking at their measurements supports our conclusions as 4 generally had high circumference measurements (e.g 19.5 for wrist) whilst 34 generally had low circumference measurements (e.g. 16.1 for wrist). Keep in mind that the range for wrist was 16.1 - 20.2. We notice that 34 seems to be an outlier in this biplot as it is quite far away from any of the other points. It's low wrist measurement may explain this.

5. In terms of the individual participant's body data, number 1 refers to the participant that has the highest body fat percentage whilst 36 has the least. On the biplot, it can be noticed that the numbers 1 and 8 are close to each other and the arrow for the variable neck. Looking at the data, 1 and 8 have similar neck measurements (i.e 41.2 for 1 and 41.4 for 8) compared to 2(neck = 38.8). Moreover, it can be argued that 4(neck = 41.3) has a similar neck measurement as 1 and 2. However, 4's knee measurement (46) is much higher compared to 1 (36.9knee) and 8 (40.9knee), thus, 4 is found closer to knee variable on the biplot. From this, we can summarise that the numbers closer to each other have similar measurement in terms of a specific variable and the closer they are to a variable on the biplot, the greater the measured value they possess for that variable. 


6.Considering the loading plot, we  observe what was already seen in the biplot, that for PC1, almost all the variables contribute equally and negatively, hence the arrows for each component point left on the Biplot and roughly along the same point on the x axis. 
For PC2, ankle knee and thigh contribute positively hence they are above the x axis on the biplot.Biceps, forearm and neck on the other hand contribute negatively hence they are below the x axis on the biplot. Once again, we notice that the loadings go from most negative to most positive in order of the body i.e. neck contributes the most negatively and ankle contributes the most positively. In the same vain, we notice that that lower body parts are contributing positively and upper body parts(with the exception of wrist) are contributing negatively.
For PC3, we observe that the fleshy parts of the body (chest, hip, neck and thigh) contribute positively and bony parts (wrist, knee, ankle) contribute negatively. Interestingly, biceps contribute the most and negatively to PC3. 

7. As PC4 only explains 5% of the variance, it is not significant.


##Biplor using PC2 and PC3 as the axes
```{r}
autoplot(pca_results,x=2, y=3, data=bodydata_9, label=TRUE, shape=FALSE,
         loadings=TRUE, loadings.label=TRUE)+
  labs(title = "PCA Biplot for the 9 circumference measurements (PC2 and PC3 as axes)")
```

1. On analysing the biplot for PC2 and PC3, we  notice that in total 19.12% of the variance is explained by the two components.

2. We notice that neck and ankle are in opposite directions horizontally which illustrates what was shown on the loadings plot (that neck contributes negatively to PC2 whilst ankle contributes positively).

3. Similar to the biplot for PC1 and PC2 knee and ankle remain close to each other indicating that they are correlated. Another observation is that, fleshy parts (chest, hip and thigh) seem to be together and, along with neck, are above the x axis, thus they contribute positively to PC3. Forearm, biceps and wrist seem to be together and they, along with knee and ankle are below the x axis, thus they contribute negatively to PC3. If we take a closer look, a physical body structure seems to be represented with upper body parts on the left of the y axis (with the exception of wrist) and lower body parts on the right. Or again, our other interpretation for PC2 is seen, fleshy parts on the left and bony parts on the right. 

3. In terms of the participant's dataset, it can be noticed that the numbers close to each other seem to have similar measurements in terms of the body measurement variable they are close to. For example, we can observe that the numbers between thigh and ankle have similar lower body measurements. Moreover, the numbers  9 and 35 are close to each other and have very similar circumference measurements in terms of wrist, forearm, hip and neck. 

4. For this biplot, 36 seems to be an outlier and this is the participant who had the lowest body fat percentage and low circumference measurements for most body parts (barring biceps). 

##Comparing equivalent results including age, weight and height. 

```{r}
#PCA using the 12 variables (9 body circumference measurements AND age, weight and height) WITHOUT scaling
pca_results = prcomp(bodydata_12)
summary(pca_results)

#Biplot PC1 AND 2 (FOR 12 VARIABLES) not scaled
autoplot(pca_results, data=bodydata_12, label=TRUE, shape=FALSE,
         loadings=TRUE, loadings.label=TRUE)+
labs(title = "PCA Biplot for the 12 variables - unscaled")
```

Once again, the differences in magnitude of measurements means that we need to scale the data. 

```{r}
#PCA using the 12 variables (9 body circumference measurements AND age, weight and height) WITH scaling
pca_results = prcomp(bodydata_12,scale.=TRUE)
summary(pca_results)

#Biplot PC1 AND 2 (FOR 12 VARIABLES) Scaled
autoplot(pca_results, data=bodydata_12, label=TRUE, shape=FALSE,
         loadings=TRUE, loadings.label=TRUE)+
  labs(title = "PCA Biplot for the 12 variables - scaled")

# Loadings plot (FOR 12 VARIABLES)
loadings = as.data.frame(pca_results$rotation[,1:4])
loadings$Symbol = row.names(loadings)
loadings = gather(loadings, key='Component', value='Weight', -Symbol)
ggplot(loadings, aes(x=Symbol,y=Weight)) +
  geom_bar(stat='identity') +
  facet_grid(Component~.)+
  labs(title = "PCA Loadings plot for the 12 variables")

#BIPLOT USING PC2 AND 3 (FOR 12 VARIABLES)
autoplot(pca_results,x=2, y=3, data=bodydata_12, label=TRUE, shape=FALSE,
         loadings=TRUE, loadings.label=TRUE) +
labs(title = "PCA Biplot for the 12 variables (PC2 and PC3 as axes)")

#Scree plot method 1
p=fviz_screeplot(pca_results, addlabels = TRUE,
                 linecolor ="red",ncp = 12)
p + labs(title = "Scree Plot for 9 Body Measurements(+ age, weight, height)(Method 1)",
         x = "Principal Components", y = "% of variance explained")

#Scree plot method 2
variance_df = data.frame(PC= paste0("PC",1:12),
                         var_explained=(pca_results$sdev)^2/sum((pca_results$sdev)^2))

head(variance_df)
variance_df %>%
  ggplot(aes(x=PC,y=var_explained, group=1))+
  geom_col()+
  geom_point(size=4)+
  geom_line()+
  labs(title="Scree Plot for 9 Body Measurements(+ age, weight, height)(Method 2)")

```


1. On adding age, height and weight, PC1 and PC2 account for 72.75% of variance explained which is about 5% less than with only 9 variables.

2. However, we also notice that height and age are near the y axis for PC2. Looking at the highest and lowest participants on the axis for PC2 (5 and 1 respectively), we see that 5 is on the taller side (71.25) and younger (24) participants whilst 1 is one of the shorter (64.0) and older (51) participants (keep in mind that height ranges from 64-76 and age ranges from 23 to 67). We also notice that weight is along the x axis for PC1. Looking at the highest and lowest participants for PC1 (4 and 34), we notice that 4 has the highest weight (227.75) in the data set whilst 34 has the lowest (125.25). So when age, weight and height are included, PC1 seems to account for weight and size whilst PC2 seems to account for height and age as well as fleshy vs bony. 

3.  On adding age, height and weight, PC2 and PC3 account for 23.5% of variance explained which is about 4.38% more than the PC2 and PC3 plot when age, height and weight were not included. We notice that age contributes negatively to both PC2 and PC3 whereas height contributes positively to PC2 but negatively to PC3.

#TASK 2 - CLUSTERING

As established earlier, we are scaling the data due to the differences in magnitude between measurements. We have tried 3 distance metrics (euclidean, manhattan and maximum) and 3 hierarchical clustering methods (ward, single and weighted).

Link for different distance metrics: https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/dist
Link for different hierarchical clustering: methods:https://www.rdocumentation.org/packages/cluster/versions/2.1.0/topics/agnes

##Clustering the body measurements (columns) only including the 9 circumference measurements
```{r}
library(cluster)

#The distances tried are euclidean, manhattan and maximum
D1 = dist(t(scale(bodydata_9)), method="euclidean")
D2 = dist(t(scale(bodydata_9)), method="manhattan")
D3 = dist(t(scale(bodydata_9)), method="maximum")

#Trying the clustering method ward with each distance metric
cluster_results1 = agnes(D1, method="ward")
cluster_results2 = agnes(D2, method="ward")
cluster_results3 = agnes(D3, method="ward")

#Euclidean and ward: agglomerative coefficient = 0.45
plot(cluster_results1,which.plots=2, main = "Dendogram for clustering body measurements (Euclidean and Ward)")
rect.hclust(cluster_results1, k=3)


#Manhattan and ward: agglomerative coefficient = 0.48
plot(cluster_results2,which.plots=2, main = "Dendogram for clustering body measurements (Manhattan and Ward)")
rect.hclust(cluster_results2, k=3)

#Maximum and ward: agglomerative coefficient = 0.52 BEST
plot(cluster_results3,which.plots=2, main = "Dendogram for clustering body measurements (Maximum and Ward)")
rect.hclust(cluster_results3, k=4, border = 2:4)

#Trying the clustering method single with each distance metric
cluster_results4 = agnes(D1, method="single")
cluster_results5 = agnes(D2, method="single")
cluster_results6 = agnes(D3, method="single")

#Euclidean and single: agglomerative coefficient = 0.23
plot(cluster_results4,which.plots=2, main = "Dendogram for clustering body measurements (Euclidean and Single)")
rect.hclust(cluster_results4, k=3)

#Manhattan and single: agglomerative coefficient = 0.19
plot(cluster_results5,which.plots=2, main = "Dendogram for clustering body measurements (Manhattan and Single)")
rect.hclust(cluster_results5, k=3)

#Maximum and single: agglomerative coefficient = 0.24
plot(cluster_results6,which.plots=2, main = "Dendogram for clustering body measurements (Maximum and Single)")
rect.hclust(cluster_results6, k=3)

#Trying the clustering method weighted with each distance metric
cluster_results7 = agnes(D1, method="weighted")
cluster_results8 = agnes(D2, method="weighted")
cluster_results9 = agnes(D3, method="weighted")

#Euclidean and weighted: agglomerative coefficient = 0.30
plot(cluster_results7,which.plots=2, main = "Dendogram for clustering body measurements (Euclidean and Weighted)")
rect.hclust(cluster_results7, k=3)

#Manhattan and weighted: agglomerative coefficient = 0.34
plot(cluster_results8,which.plots=2, main = "Dendogram for clustering body measurements (Manhattan and Weighted)")
rect.hclust(cluster_results8, k=3)

#Maximum and weighted: agglomerative coefficient = 0.43
plot(cluster_results9,which.plots=2, main = "Dendogram for clustering body measurements (Maximum and Weighted)")
rect.hclust(cluster_results9, k=3)
```

1. The agglomerative coefficients were lowest when we used the clustering method "single" and highest when we used the clustering method "ward". 

2. We found that the distance metric "Maximum" with the clustering method "ward" gave the highest agglomerative coefficient of 0.52, thus this was the best of the methods tried. 

3. From this dendogram, we can see that chest and hip are most similar as they are clustered at the lowest height. We also notice that the thigh and knee are clustered at quite a low height and ankle joins the cluster shortly after. We notice that these 3 are lower body measurements whereas the other cluster (neck, forearm and wrist) are upper body measurements. 

5. We also notice that biceps is the most dissimilar. We wonder if this may be because biceps can be large on people who have a lot of body fat and may be overweight but also on people who may work out and have low body fat. The latter people could be fit/trim and not have large circumference measurements for other body parts but may have large biceps from trying to build muscle. 

##Clustering the people (rows) only including the 9 circumference measurements
```{r}
#------------------Cluster ROWS (people) for 9 variables-----------------------------
D1 = dist(scale(bodydata_9), method="euclidean")
D2 = dist(scale(bodydata_9), method="manhattan")
D3 = dist(scale(bodydata_9), method="maximum")

#Trying the clustering method ward with each distance metric
cluster_results1 = agnes(D1, method="ward")
cluster_results2 = agnes(D2, method="ward")
cluster_results3 = agnes(D3, method="ward")

#Euclidean and ward: agglomerative coefficient = 0.87
plot(cluster_results1,which.plots=2, main = "Dendogram for clustering people (Euclidean and Ward)")
rect.hclust(cluster_results1, k=10)


#Manhattan and ward: agglomerative coefficient = 0.89 BEST!!!!!
plot(cluster_results2,which.plots=2, main = "Dendogram for clustering people (Manhattan and Ward)")
rect.hclust(cluster_results2, k=10, border = 2:10)

#Maximum and ward: agglomerative coefficient = 0.83
plot(cluster_results3,which.plots=2, main = "Dendogram for clustering people (Maximum and Ward)")
rect.hclust(cluster_results3, k=10)

#Trying the clustering method single with each distance metric
cluster_results4 = agnes(D1, method="single")
cluster_results5 = agnes(D2, method="single")
cluster_results6 = agnes(D3, method="single")

#Euclidean and single: agglomerative coefficient = 0.48
plot(cluster_results4,which.plots=2, main = "Dendogram for clustering people (Euclidean and Single)")
rect.hclust(cluster_results4, k=10)

#Manhattan and single: agglomerative coefficient = 0.46
plot(cluster_results5,which.plots=2, main = "Dendogram for clustering people (Manhattan and Single)")
rect.hclust(cluster_results5, k=10)

#Maximum and single: agglomerative coefficient = 0.48
plot(cluster_results6,which.plots=2, main = "Dendogram for clustering people (Maximum and Single)")
rect.hclust(cluster_results6, k=10)

#Trying the clustering method weighted with each distance metric
cluster_results7 = agnes(D1, method="weighted")
cluster_results8 = agnes(D2, method="weighted")
cluster_results9 = agnes(D3, method="weighted")

#Euclidean and weighted: agglomerative coefficient = 0.75
plot(cluster_results7,which.plots=2, main = "Dendogram for clustering people (Euclidean and Weighted)")
rect.hclust(cluster_results7, k=10)

#Manhattan and weighted: agglomerative coefficient = 0.78
plot(cluster_results8,which.plots=2, main = "Dendogram for clustering people (Manhattan and Weighted)")
rect.hclust(cluster_results8, k=10)

#Maximum and weighted: agglomerative coefficient = 0.7
plot(cluster_results9,which.plots=2, main = "Dendogram for clustering people (Maximum and Weighted)")
rect.hclust(cluster_results9, k=10)
```

1. Once again, we found that the clustering method "single" produced the lowest agglomerative coefficients whilst "ward" produced the highest

2.On comparing the clusters for 9 variables, we can observe that the distance metric "Manhattan" and clustering method "ward" gives the highest agglomerative coefficient of 0.89.

3. We obtain 10 cluster groups in this dendogram.On observing the first cluster containing 1,2,4 we can notice similarity in some of their body circumference measurements such as chest, hip, thigh, ankle, biceps and wrist. The same can be observed for the 3rd cluster containing 14,22,20,33 where all of these have most of the body circumference measurements similar.To summarise, we notice that in each cluster, the individuals have similar body measurements. 

##Clustering the variables (columns) including the 12 varaibles (the 9 circumference measurements as well as age, weight and height).

```{r}
#The distances tried are euclidean, manhattan and maximum
D1 = dist(t(scale(bodydata_12)), method="euclidean")
D2 = dist(t(scale(bodydata_12)), method="manhattan")
D3 = dist(t(scale(bodydata_12)), method="maximum")

#Trying the clustering method ward with each distance metric
cluster_results1 = agnes(D1, method="ward")
cluster_results2 = agnes(D2, method="ward")
cluster_results3 = agnes(D3, method="ward")

#Euclidean and ward: agglomerative coefficient = 0.54
plot(cluster_results1,which.plots=2, main = "Dendogram for clustering body measurements (Euclidean and Ward)")
rect.hclust(cluster_results1, k=6)

#Manhattan and ward: agglomerative coefficient = 0.54
plot(cluster_results2,which.plots=2, main = "Dendogram for clustering body measurements (Manhattan and Ward)")
rect.hclust(cluster_results2, k=6)

#Maximum and ward: agglomerative coefficient = 0.58 BEST
plot(cluster_results3,which.plots=2, main = "Dendogram for clustering body measurements (Maximum and Ward)")
rect.hclust(cluster_results3, k=6, border = 2:6)


#Trying the clustering method single with each distance metric
cluster_results4 = agnes(D1, method="single")
cluster_results5 = agnes(D2, method="single")
cluster_results6 = agnes(D3, method="single")

#Euclidean and single: agglomerative coefficient = 0.38
plot(cluster_results4,which.plots=2, main = "Dendogram for clustering body measurements (Euclidean and Single)")
rect.hclust(cluster_results4, k=6)

#Manhattan and single: agglomerative coefficient = 0.4
plot(cluster_results5,which.plots=2, main = "Dendogram for clustering body measurements (Manhattan and Single)")
rect.hclust(cluster_results5, k=6)

#Maximum and single: agglomerative coefficient = 0.34
plot(cluster_results6,which.plots=2, main = "Dendogram for clustering body measurements (Maximum and Single)")
rect.hclust(cluster_results6, k=6)

#Trying the clustering method weighted with each distance metric
cluster_results7 = agnes(D1, method="weighted")
cluster_results8 = agnes(D2, method="weighted")
cluster_results9 = agnes(D3, method="weighted")

#Euclidean and weighted: agglomerative coefficient = 0.46
plot(cluster_results7,which.plots=2, main = "Dendogram for clustering body measurements (Euclidean and Weighted)")
rect.hclust(cluster_results7, k=6)

#Manhattan and weighted: agglomerative coefficient = 0.49
plot(cluster_results8,which.plots=2, main = "Dendogram for clustering body measurements (Manhattan and Weighted)")
rect.hclust(cluster_results8, k=6)

#Maximum and weighted: agglomerative coefficient = 0.44
plot(cluster_results9,which.plots=2, main = "Dendogram for clustering body measurements (Maximum and Weighted)")
rect.hclust(cluster_results9, k=6)
```

1. Generally, adding age,height and weight increased the agglomerative coefficient for clustering by columns (body measurements)

2. Once again, the distance metric "Maximum" and the clustering method "ward" produced the highest agglomerative coefficient (0.58) when clustering by body measurements (but now with age, height and weight included). 

3. We find that weight is clustered early on with hip and later joined by chest. This may be because the chest and hip size are usually indicators of a persons weight (i.e. people who weigh more generally have larger chests and hips). 

4. We notice that age is quite dissimilar to a lot of the variables. This may be because, as aforementioned, people of different ages can be different sizes. This may also explain why height is clustered at a high height too i.e. both short and tall people can have high or low body fat/body circumference measurements. 

5. Once again, we notice that biceps is the most dissimilar and this may also be because people who have high or low body fat percentage can have large biceps. We think this might be the case with 36, the individual with the lowest body fat percentage but the largest biceps. This man may be a muscular person. 

##Clustering the people (rows) including the 12 varaibles (the 9 circumference measurements as well as age, weight and height).
```{r}
D1 = dist(scale(bodydata_12), method="euclidean")
D2 = dist(scale(bodydata_12), method="manhattan")
D3 = dist(scale(bodydata_12), method="maximum")

#Trying the clustering method ward with each distance metric
cluster_results1 = agnes(D1, method="ward")
cluster_results2 = agnes(D2, method="ward")
cluster_results3 = agnes(D3, method="ward")

#Euclidean and ward: agglomerative coefficient = 0.85
plot(cluster_results1,which.plots=2, main = "Dendogram for clustering people (Euclidean and Ward)")
rect.hclust(cluster_results1, k=10)

#Manhattan and ward: agglomerative coefficient = 0.88 BEST!!!!!
plot(cluster_results2,which.plots=2, main = "Dendogram for clustering people (Manhattan and Ward)")
rect.hclust(cluster_results2, k=10, border = 2:10)

#Maximum and ward: agglomerative coefficient = 0.79
plot(cluster_results3,which.plots=2, main = "Dendogram for clustering people (Maximum and Ward)")
rect.hclust(cluster_results3, k=10)

#Trying the clustering method single with each distance metric
cluster_results4 = agnes(D1, method="single")
cluster_results5 = agnes(D2, method="single")
cluster_results6 = agnes(D3, method="single")

#Euclidean and single: agglomerative coefficient = 0.39
plot(cluster_results4,which.plots=2, main = "Dendogram for clustering people (Euclidean and Single)")
rect.hclust(cluster_results4, k=10)

#Manhattan and single: agglomerative coefficient = 0.38
plot(cluster_results5,which.plots=2, main = "Dendogram for clustering people (Manhattan and Single)")
rect.hclust(cluster_results5, k=5)

#Maximum and single: agglomerative coefficient = 0.42
plot(cluster_results6,which.plots=2, main = "Dendogram for clustering people (Maximum and Single)")
rect.hclust(cluster_results6, k=10)

#Trying the clustering method weighted with each distance metric
cluster_results7 = agnes(D1, method="weighted")
cluster_results8 = agnes(D2, method="weighted")
cluster_results9 = agnes(D3, method="weighted")

#Euclidean and weighted: agglomerative coefficient = 0.67
plot(cluster_results7,which.plots=2, main = "Dendogram for clustering people (Euclidean and Weighted)")
rect.hclust(cluster_results7, k=10)

#Manhattan and weighted: agglomerative coefficient = 0.74
plot(cluster_results8,which.plots=2, main = "Dendogram for clustering people (Manhattan and Weighted)")
rect.hclust(cluster_results8, k=10)

#Maximum and weighted: agglomerative coefficient = 0.66
plot(cluster_results9,which.plots=2, main = "Dendogram for clustering people (Maximum and Weighted)")
rect.hclust(cluster_results9, k=10)
```

DISCUSS FINDINGS HERE

1. Generally, adding age,height and weight increased the agglomerative coefficient for clustering by columns (body measurements) but here, when clustering by rows (people), adding age height and weight has reduced the agglomerative coefficients. 

2. Once again, the distance metric "Manhattan" and the clustering method "ward" has given the highest agglomerative coefficient (0.88) for clustering by people.

3. On first glance, the clusters look quite similar to when age, weight and height were not included. However, on further inspection, when age height and weight are included, the height at which all the clusters meet is greater by about 10. Furthermore, the smaller clusters seem to have remained similar but seem to join at different heights now. For example, when age, height and weight were NOT included, the cluster including 14,22,20 and 33 joined the cluster including 1,2,4, and 8 at around height 12 but when age, height and weight are included, these clusters do not meet until a height around 32. This shows that adding age, height and weight changes the way in which the clusters are formed. 


#Task 3

##Assessing the methods applied and Overall conclsions from PCA and Clustering 

Principal Component Analysis (PCA) is a widely used exploratory technique for dimension reduction of multidimensional data sets. It helps by removing correlated features so that each principal component is independent of the others. This makes the multidimensional data set simpler which is then easier to explore and visualise. We saw this with our data set as we were able to visualise the different facets of the body data set on the biplot, exploring both the variables (body measurements) and observations (people). However, some disadvantages of PCA is that principal components are not as interpretable as the original variables and some information can be lost.

https://www.i2tutorials.com/what-are-the-pros-and-cons-of-the-pca/

Clustering is also and exploratory technique but is used as a method of data segmentation. it groups the data into several clusters or groups based on their similarities. In this task, we have used hierarchical clustering. In addition to helping to find similarities between variables and observations, another advantage of clustering is that it can be used to identify both outliers and dissimilarities in the data set. However, some disadvantages of hierarchical clustering include the fact that arbitrary decisions have to be made to choose the distance metric and linkage criteria. For example we have used different distance metrics such as euclidean, manhattan and maximum on the 9 and 12 variables along with various linkage methods (single, ward and weighted) in an attempt to find the best combination that produced the highest the agglomerative coefficient. Some other disadvantages include the fact that hierarchical clustering often does not work well on large data sets and works poorly with mixed data types. However, this was not an issue we faced given that this data set was small and the data was all numeric. Our research suggests that latent class analysis is a modern technique that addresses all the issues identified with hierarchical clustering. It would be interesting to see the outputs of it with this data set. 

https://www.displayr.com/strengths-weaknesses-hierarchical-clustering/

##Drawing overall conclusions
Drawing overall conclusions, we have found that applying PCA and hierarchical clustering are means of exploring the data set and trying to see what is happening. We conclude that there can be various explanations for observations e.g. PC1 may be viewed as a measure of the average size of a man whilst PC2 may be viewed as a measure of how fleshy or bony a man is or upper and lower body measurements. There is no "right" answer and these views help us to get a better sense of the data. From clustering, we can conclude that the participants with the most similar measurements are grouped together. We also conclude that age, height and bicep are less similar to other variables and this may be because for each of these variables, body size and body fat percentage can vary at each extreme of the variable. i.e. both older and younger and taller or shorter men can have a high body fat percentage and size measurements. On the other hand, both overweight and muscular men can have large biceps but the muscular man may not have large measurements in other parts of his body as the overweight man might. 


#Similarities and conflicts in insights gained from PCA and clustering
Exploring the data set using PCA and hierarchical clustering has been insightful for us as a group. We were able to identify ways in which the methods alluded to similar things but also ways in which they conflicted. We have repeated the graphs we are about to refer to below for ease of comparison. A similarity we noticed was the fact that the people in the same clusters were found close to each other in the biplots. This indicates that both methods place people close to each other based on having similar measurements for variables. As seen in the dendogram where people are clustered, 14 and 22 are clusted together at a low height and shortly joined by 20 and 33. Looking at the PCA biplot, we notice that 14 and 22 are close to eachother and also to 20 and 33. We also noticed that on the PCA biplot, 34 seemed to be an outlier as it was on it own far away from all the other points. Similarly in the dendogram for clustering by people, 34 is the last individual to join a cluster supporting that it may be an outlier and would be worth investigating further. 

A conflict we observed was that in the PCA biplot, the variable bicep was close to forearm and neck but on  the dendogram for clustering by variables, it was the most dissimilar and was the last variable to join the clusters.

```{r}
pca_results = prcomp(bodydata_9,scale.=TRUE)
summary(pca_results)

#Biplot
autoplot(pca_results, data=bodydata_9, label=TRUE, shape=FALSE,
         loadings=TRUE, loadings.label=TRUE)+
labs(title = "PCA Biplot for the 9 circumference measurements - scaled")

#Dendogram for clustering by body measurements
D3 = dist(t(scale(bodydata_9)), method="maximum")
cluster_results3 = agnes(D3, method="ward")
#Maximum and ward: agglomerative coefficient = 0.52 BEST
plot(cluster_results3,which.plots=2, main = "Dendogram for clustering body measurements (Maximum and Ward)")
rect.hclust(cluster_results3, k=4, border = 2:4)

#Dendogram for clustering by
D2 = dist(scale(bodydata_9), method="manhattan")
cluster_results2 = agnes(D2, method="ward")
#Manhattan and ward: agglomerative coefficient = 0.89 BEST!!!!!
plot(cluster_results2,which.plots=2, main = "Dendogram for clustering people (Manhattan and Ward)")
rect.hclust(cluster_results2, k=10, border = 2:10)


```


When including the variables age, height and weight in our exploratory data analysis ([plots below), we again noticed some similarities and conflicts between PCA and hierarchical clustering. A similarity we found was that in the PCA biplot, age and height were almost opposite to each other. Similarly, the dendogram indicates that age and height have little similarity as they do not join the same cluster until much higher up. Another similarity we found was that weight was close to hip in both the dendogram and the PCA biplot.
```{r}
pca_results = prcomp(bodydata_12,scale.=TRUE)
summary(pca_results)

#Biplot
autoplot(pca_results, data=bodydata_12, label=TRUE, shape=FALSE,
         loadings=TRUE, loadings.label=TRUE)+
labs(title = "PCA Biplot for the 9 circumference measurements plus age, weight and height- scaled")

#Dendogram for clustering by body measurements
D3 = dist(t(scale(bodydata_12)), method="maximum")
cluster_results3 = agnes(D3, method="ward")
#Maximum and ward: agglomerative coefficient = 0.52 BEST
plot(cluster_results3,which.plots=2, main = "Dendogram for clustering body measurements (Maximum and Ward)")
rect.hclust(cluster_results3, k=6, border = 2:6)

#Dendogram for clustering by
D2 = dist(scale(bodydata_12), method="manhattan")
cluster_results2 = agnes(D2, method="ward")
#Manhattan and ward: agglomerative coefficient = 0.89 BEST!!!!!
plot(cluster_results2,which.plots=2, main = "Dendogram for clustering people (Manhattan and Ward)")
rect.hclust(cluster_results2, k=10, border = 2:10)


```
Finaly, we tried clustering by principal components and found that...

```{r}
pca_results$rotation
D3 = dist(scale(pca_results$rotation), method="maximum")
cluster_results3 = agnes(D3, method="ward")
#Maximum and ward: agglomerative coefficient = 0.79
plot(cluster_results3,which.plots=2, main = "Dendogram for clustering principal components (Maximum and Ward)")
rect.hclust(cluster_results3, k=5, border = 2:5)
```

##Communicating conclusions to participants

Communication can be done via any mode, (email, post, phone calls, in person and etc). The most important thing to do is to thank the participants for taking part in the medical study in the first place, as it is very important to ensure participants feel valued. It is also important to ensure participants feel they are able to ask any questions they might have regarding the study, their data and the findings. 

In order to communicate our conclusions from PCA and clustering with participants, we could explain that generally, people with higher body size measurements tend to have more body fat percentage and that this can apply across all ages and heights. Showing the PCA biplot or the clustering dendogram may not be necessary as this could be confusing rather than helpful to someone who is not a data scientist. 

It would be important to communicate with the participants their current body fat percentage and what this means in terms of their health, for example if they may be considered over, normal or under weight based on their measurements. Letting the participants know what they need to watch out for e.g. changes in their weight could be helpful. For example, if they notice their weight has either dropped significantly or they have gained excessive weight, they should speak to their medical professional about this. Any such changes could mean nothing at all, or could mean the fat in the body has gone below or above the "normal" range for body fat percentage. Therefore it is important to get checked when there are significant noticeable changes. It is also important to let the participants know and understand that body fat balance is required for good health as too little body fat could lead to Anorexia, or too much could lead to obesity hence the need to always keep an eye on changes and seeking help from the doctors if there are any concerns.


















